# -*- coding: utf-8 -*-
"""dataset 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hVb-LgK7GsUNv1iB7O2M3yzG0pjuR-cg

Imports
"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
import random
import os
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from joblib import dump, load
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

"""cic ids 2017 my beloved <3"""

!unzip CIC_IDS_2017.zip

"""carregar dados"""

df_list = []
for file in os.listdir('MachineLearningCVE/'):
  df_aux = pd.read_csv(f'MachineLearningCVE/{file}')
  df_list.append(df_aux)
df = pd.concat(df_list, ignore_index=True)

"""limpar dados"""

df.columns = df.columns.str.strip()

initial_len = df.shape[0]
df = df.drop_duplicates()
print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartadas {initial_len - df.shape[0]} duplicadas')

initial_len = df.shape[0]
df = df.dropna()
print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartados {initial_len - df.shape[0]} registros com valores NA')

df = df.reset_index(drop=True)

max_finite_flow_packets_per_sec = df[np.isfinite(df['Flow Packets/s'])]['Flow Packets/s'].max()
max_finite_flow_bytes_per_sec = df[np.isfinite(df['Flow Bytes/s'])]['Flow Bytes/s'].max()

df.loc[df['Flow Packets/s'] == np.inf, 'Flow Packets/s'] = max_finite_flow_packets_per_sec
df.loc[df['Flow Bytes/s'] == np.inf, 'Flow Bytes/s'] = max_finite_flow_bytes_per_sec

df['Label'] = df['Label'].replace({'Web Attack � Brute Force':'Brute Force', 'Web Attack � XSS':'XSS', 'Web Attack � Sql Injection':'Sql Injection'})

"""train test split (sem val(com val))"""

df_train = df.sample(frac=0.7, random_state=RANDOM_SEED)
df_val_test = df.drop(df_train.index)

df_train = df_train.reset_index(drop=True)
df_val_test = df_val_test.reset_index(drop=True)

y_train = df_train['Label']
X_train = df_train.drop('Label', axis='columns')

X_val, X_test, classes_val, classes_test = train_test_split(df_val_test.drop('Label', axis='columns'), df_val_test['Label'], test_size=0.65, stratify=df_val_test['Label'], random_state=RANDOM_SEED)

X_val, X_test = X_val.reset_index(drop=True), X_test.reset_index(drop=True)
classes_val, classes_test =  classes_val.reset_index(drop=True), classes_test.reset_index(drop=True)

y_val, y_test = classes_val.apply(lambda c: 0 if c == 'BENIGN' else 1), classes_test.apply(lambda c: 0 if c == 'BENIGN' else 1)
y_train = y_train.apply(lambda c: 0 if c == 'BENIGN' else 1)

del df_train, df_val_test, df

"""limpar dados pt 2"""

def get_highly_correlated_features(correlation_matrix, threshold):
  correlated_pairs = []
  for i in range(len(correlation_matrix.columns)):
    for j in range(i):
      if abs(correlation_matrix.iloc[i, j]) > threshold:
        pair = (correlation_matrix.columns[i], correlation_matrix.columns[j])
        coefficient = correlation_matrix.iloc[i, j]
        correlated_pairs.append((pair, coefficient))
  return sorted(correlated_pairs, key= lambda pair: pair[1], reverse=True)

corr_matrix = X_train.corr().abs()
correlation_list = get_highly_correlated_features(corr_matrix, 0.95)

f2drop = []
for feature_pair, _ in correlation_list:
  if feature_pair[0] not in f2drop and feature_pair[1] not in f2drop:
    f2drop.append(feature_pair[1])

f2drop = f2drop + ['Destination Port']

X_train = X_train.drop(f2drop, axis='columns')
X_val = X_val.drop(f2drop, axis='columns')
X_test = X_test.drop(f2drop, axis='columns')

"""scaling"""

minmax_scaler = MinMaxScaler()
minmax_scaler = minmax_scaler.fit(X_train)

norm_X_train = minmax_scaler.transform(X_train)
norm_X_val = minmax_scaler.transform(X_val)
norm_X_test = minmax_scaler.transform(X_test)

del X_train, X_val, X_test

norm_X_test = pd.DataFrame(norm_X_test)
norm_X_val = pd.DataFrame(norm_X_val)

"""val :'("""

norm_X_test = pd.concat([norm_X_test, norm_X_val], ignore_index=True)
y_test = pd.concat([y_test, y_val], ignore_index=True)
del norm_X_val, y_val

"""SMOTE dos dados maliciosos (duas classes)"""

sm = SMOTE(random_state=RANDOM_SEED)

norm_X_train, y_train = sm.fit_resample(norm_X_train, y_train)

"""treinar"""

rf1 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced'
    )

rf2 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced'
    )

rf3 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced'
    )

clf = StackingClassifier(estimators=[('rf1', rf1), ('rf2', rf2), ('rf3', rf3)], cv=10, final_estimator=LogisticRegression(max_iter=400))

clf.fit(norm_X_train, y_train)

#save point
clf = load("/content/modelo2.joblib")

clf.score(norm_X_test, y_test)

y_pred = clf.predict(norm_X_test)

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average="macro")
rec = recall_score(y_test, y_pred, average="macro")
prec = precision_score(y_test, y_pred, average="macro")
print(f"acc: {acc} f1: {f1} rec: {rec} prec: {prec}")

"""Explainer"""

!pip install explainerdashboard

from explainerdashboard import ClassifierExplainer, ExplainerDashboard, InlineExplainer

!pip install shap==0.44.1

X_lol, X_test_small, classes_lol, y_test_small = train_test_split(norm_X_test, y_test, test_size=0.01, stratify=y_test, random_state=RANDOM_SEED)

class_explainer = ClassifierExplainer(clf, X_test_small, y_test_small)

"""0 = benign
1 = malicious
"""

class_explainer.plot_confusion_matrix()

InlineExplainer(class_explainer).classifier.roc_auc()

from joblib import dump, load
dump(clf, 'modelo2.joblib')

"""300 horas :)"""

class_explainer.plot_contributions(index=10)

class_explainer.plot_importances()

"""Explicar apenas uma floresta usando explainer específico de árvores (geral é bem mais lento)"""

rf4 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced'
    )

rf4.fit(norm_X_train, y_train)

fast_explainer = ClassifierExplainer(rf4, norm_X_test, y_test.ravel())

"""Choro e ranger de dentes pt 2"""

fast_explainer.plot_contributions(index=0)

fast_explainer.plot_importances()