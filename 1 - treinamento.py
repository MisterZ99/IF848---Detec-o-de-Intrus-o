# -*- coding: utf-8 -*-
"""Treinamento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reBWzQmHe4vAzLxr6sN1xyaP1xwkuAA8
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import random
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import GridSearchCV
from google.colab import drive
drive.mount('/content/drive')
from joblib import dump, load
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

!pip install explainerdashboard

from explainerdashboard import ClassifierExplainer, ExplainerDashboard, InlineExplainer

!pip install shap==0.44.1

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

"""Carregar dados de treino"""

#files.upload() #firefox glitch workaround

df1 = pd.read_csv("train_data1.csv")
df2 = pd.read_csv("train_data2.csv")
df3 = pd.read_csv("train_data3.csv")

"""brincar com o modelo / validação freestyle"""

#y = df1['Label']
#X = df1.drop(['Label'], axis=1)

#X_train, X_val,y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42)

#rf = RandomForestClassifier(
#    random_state=RANDOM_SEED,
#    n_estimators=10,
#    class_weight='balanced',
#    max_depth=5,
#    max_features="sqrt",
#    min_samples_leaf=0.00195,
#    min_samples_split=2
#    )

#rf.fit(X_train, y_train)

#rf.score(X_val, y_val)

"""Validar modelo"""

y_train1 = df1['Label']
X_train1 = df1.drop(['Label'], axis=1)
y_train2 = df2['Label']
X_train2 = df2.drop(['Label'], axis=1)
y_train3 = df3['Label']
X_train3 = df3.drop(['Label'], axis=1)
y_train1 = y_train1.apply(lambda c: 0 if c == 'Non-DoH' else (1 if c == 'Benign' else 2))
y_train2 = y_train2.apply(lambda c: 0 if c == 'Non-DoH' else (1 if c == 'Benign' else 2))
y_train3 = y_train3.apply(lambda c: 0 if c == 'Non-DoH' else (1 if c == 'Benign' else 2))

del df1, df2, df3

rf1 = RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=10, class_weight='balanced')
rf2 = RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=10, class_weight='balanced')
rf3 = RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=10, class_weight='balanced')
params = {
  'max_features': [24, 25, 26],
  'min_samples_leaf': [0.0018, 0.0019, 0.002],
  'min_samples_split': [0.0057, 0.0058, 0.0059]
  }

"""Só rodar para achar hiperparametros (não precisa para treinar)"""

grid1 = GridSearchCV(estimator=rf1,
                    param_grid=params,
                    cv=5,
                    verbose=10)
grid2 = GridSearchCV(estimator=rf2,
                    param_grid=params,
                    cv=5,
                    verbose=10)
grid3 = GridSearchCV(estimator=rf3,
                    param_grid=params,
                    cv=5,
                    verbose=10)

grid1.fit(X_train1, y_train1)
print(grid1.best_params_)

grid2.fit(X_train2, y_train2)
print(grid2.best_params_)

grid3.fit(X_train3, y_train3)
print(grid3.best_params_)

"""Treinar modelo"""

rf1 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=24,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )
rf2 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=24,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )
rf3 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=24,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )

rf1.fit(X_train1, y_train1)

rf2.fit(X_train2, y_train2)

rf3.fit(X_train3, y_train3)

clf = StackingClassifier(estimators=[('rf1', rf1), ('rf2', rf2), ('rf3', rf3)], cv='prefit', final_estimator=LogisticRegression(max_iter=400))#solver="newton-cg"))

clf.fit(X_train1, y_train1)

"""clf2 treinado em split 1 e 2"""

clf2 = StackingClassifier(estimators=[('rf1', rf1), ('rf2', rf2), ('rf3', rf3)], cv='prefit', final_estimator=LogisticRegression(max_iter=400, warm_start=True))

clf2.fit(X_train1, y_train1)

clf2.fit(X_train2, y_train2)

"""clf3 treinado em split 1, 2 e 3"""

clf3 = StackingClassifier(estimators=[('rf1', rf1), ('rf2', rf2), ('rf3', rf3)], cv='prefit', final_estimator=LogisticRegression(max_iter=400, warm_start=True))

clf3.fit(X_train1, y_train1)

clf3.fit(X_train2, y_train2)

clf3.fit(X_train3, y_train3)

"""Testar modelo"""

tdf = pd.read_csv("test_data.csv")

y_test = tdf['Label']
X_test = tdf.drop(['Label'], axis=1)
y_test = y_test.apply(lambda c: 0 if c == 'Non-DoH' else (1 if c == 'Benign' else 2))

clf.score(X_test, y_test)

clf2.score(X_test, y_test)

clf3.score(X_test, y_test)

y_pred1 = clf.predict(X_test)
y_pred2 = clf2.predict(X_test)
y_pred3 = clf3.predict(X_test)

acc1 = accuracy_score(y_test, y_pred1)
f11 = f1_score(y_test, y_pred1, average="macro")
rec1 = recall_score(y_test, y_pred1, average="macro")
prec1 = precision_score(y_test, y_pred1, average="macro")
print(f"1 acc: {acc1} f1: {f11} rec: {rec1} prec: {prec1}")

acc2 = accuracy_score(y_test, y_pred2)
f12 = f1_score(y_test, y_pred2, average="macro")
rec2 = recall_score(y_test, y_pred2, average="macro")
prec2 = precision_score(y_test, y_pred2, average="macro")
print(f"2 acc: {acc2} f1: {f12} rec: {rec2} prec: {prec2}")

acc3 = accuracy_score(y_test, y_pred3)
f13 = f1_score(y_test, y_pred3, average="macro")
rec3 = recall_score(y_test, y_pred3, average="macro")
prec3 = precision_score(y_test, y_pred3, average="macro")
print(f"3 acc: {acc3} f1: {f13} rec: {rec3} prec: {prec3}")

"""¯\\\_(ツ)\_/¯

Explainer

Choro e ranger de dentes
"""

clf = load("/content/clf1.joblib")

X_lol, X_explain, y_lol, y_explain = train_test_split(X_test, y_test, test_size=0.1, stratify=y_test, random_state=RANDOM_SEED)

del X_lol, y_lol

#plot
#class_explainer = ClassifierExplainer(clf, X_test, y_test)
#explain in a reasonable amount of time
class_explainer = ClassifierExplainer(clf, X_explain, y_explain, pos_label=2, labels=["Non-DoH", "Benign", "Malicious"])

"""Here be dragons"""

class_explainer.plot_confusion_matrix()

InlineExplainer(class_explainer).classifier.roc_auc()

class_explainer.plot_importances()

class_explainer.plot_importances_detailed()

dump(class_explainer, 'explainer.joblib')

!cp explainer.joblib /content/drive/MyDrive

#_ = ExplainerDashboard(class_explainer)
#não

"""melhoria [*citation needed*]"""

drop_features = ["ResponseTimeTimeStandardDeviation",
                 "ResponseTimeTimeSkewFromMode",
                 "ResponseTimeTimeMean",
                 "FlowSentRate",
                 "PacketTimeStandardDeviation",
                 "PacketTimeSkewFromMedian",
                 "PacketTimeSkewFromMode",
                 "PacketLengthSkewFromMode",
                 "PacketLengthVariance",
                 "PacketTimeVariance",
                 "PacketLengthMode"]
X_test_small = X_test.drop(drop_features, axis=1)
X_train1_small = X_train1.drop(drop_features, axis=1)
X_train2_small = X_train2.drop(drop_features, axis=1)
X_train3_small = X_train3.drop(drop_features, axis=1)

rfs1 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=18,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )
rfs2 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=18,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )
rfs3 = RandomForestClassifier(
    random_state=RANDOM_SEED,
    n_estimators=10,
    max_depth=5,
    class_weight='balanced',
    max_features=18,
    min_samples_leaf=0.0018,
    min_samples_split=0.0058
    )

rfs1.fit(X_train1_small, y_train1)

rfs2.fit(X_train2_small, y_train2)

rfs3.fit(X_train3_small, y_train3)

clfs = StackingClassifier(estimators=[('rfs1', rfs1), ('rfs2', rfs2), ('rfs3', rfs3)], cv='prefit', final_estimator=LogisticRegression(max_iter=400))

clfs.fit(X_train1_small, y_train1)

clfs.score(X_test_small, y_test)

y_preds = clfs.predict(X_test_small)

accs = accuracy_score(y_test, y_preds)
f1s = f1_score(y_test, y_preds, average="macro")
recs = recall_score(y_test, y_preds, average="macro")
precs = precision_score(y_test, y_preds, average="macro")
print(f"s acc: {accs} f1: {f1s} rec: {recs} prec: {precs}")

dump(clf, 'clf1.joblib')
dump(clf2, 'clf2.joblib')
dump(clf3, 'clf3.joblib')
dump(clfs, 'clfs.joblib')

small_explainer = ClassifierExplainer(clfs, X_test_small, y_test)

InlineExplainer(small_explainer).classifier.roc_auc()