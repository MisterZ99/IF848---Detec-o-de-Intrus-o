# -*- coding: utf-8 -*-
"""Preprocessamento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16BlRcmhyORbPdLCQ0Eg69102NF3yEPvr

Imports
"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
import random

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

"""Carregar dados não-maliciosos"""

df = pd.read_csv("/content/all.csv") #chrome
df_aux = pd.read_csv("/content/all-firefox.csv")
df_list = [df, df_aux]
df = pd.concat(df_list, ignore_index=True)

del df_list, df_aux

df.loc[df.DoH, "Label"] = "Benign"

values = {"Label":"Non-DoH"}
df = df.fillna(value = values)

"""Carregar dados maliciosos"""

mdf = pd.read_csv("/content/all-mal-1.csv")
mdf_aux = pd.read_csv("/content/all-mal-2.csv")
mdf_aux_sq = pd.read_csv("/content/all-mal-3.csv")
mdf_list = [mdf, mdf_aux, mdf_aux_sq]
mdf = pd.concat(mdf_list, ignore_index=True)

del mdf_aux, mdf_aux_sq, mdf_list

mdf["Label"] = "Malicious"

df = pd.concat([df, mdf], ignore_index=True)
del mdf

"""dropar features irrelevantes e limpar dados"""

df = df.drop(["SourceIP", "DestinationIP", "SourcePort", "DestinationPort", "TimeStamp", "DoH"], axis=1)

df.columns = df.columns.str.strip()

initial_len = df.shape[0]
df = df.dropna()
print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartados {initial_len - df.shape[0]} registros com valores NA')

df = df.reset_index(drop=True)

"""Scaling dos dados usando minmax"""

scaler = MinMaxScaler()
scaler.fit(df.drop(["Label"], axis=1))

df_aux = scaler.transform(df.drop(["Label"], axis=1))

df_aux = pd.DataFrame(df_aux, columns=['Duration', 'FlowBytesSent', 'FlowSentRate', 'FlowBytesReceived',
       'FlowReceivedRate', 'PacketLengthVariance',
       'PacketLengthStandardDeviation', 'PacketLengthMean',
       'PacketLengthMedian', 'PacketLengthMode', 'PacketLengthSkewFromMedian',
       'PacketLengthSkewFromMode', 'PacketLengthCoefficientofVariation',
       'PacketTimeVariance', 'PacketTimeStandardDeviation', 'PacketTimeMean',
       'PacketTimeMedian', 'PacketTimeMode', 'PacketTimeSkewFromMedian',
       'PacketTimeSkewFromMode', 'PacketTimeCoefficientofVariation',
       'ResponseTimeTimeVariance', 'ResponseTimeTimeStandardDeviation',
       'ResponseTimeTimeMean', 'ResponseTimeTimeMedian',
       'ResponseTimeTimeMode', 'ResponseTimeTimeSkewFromMedian',
       'ResponseTimeTimeSkewFromMode',
       'ResponseTimeTimeCoefficientofVariation'])

df = pd.concat([df_aux, df['Label']], axis=1)

del df_aux

"""train test split :)"""

temp_labels = df['Label']
df = df.drop(['Label'], axis=1)

df_train, df_test, labels_train, labels_test = train_test_split(df, temp_labels, test_size=0.1, stratify=temp_labels, random_state=RANDOM_SEED)

df_train = pd.concat([df_train, labels_train], axis=1)

df_test = pd.concat([df_test, labels_test], axis=1)

"""SMOTE upsampling dos dados benignos"""

df = df_train
del df_train

ben = df.loc[df['Label'] == "Benign"]
non = df.loc[df['Label'] == "Non-DoH"]
mal = df.loc[df['Label'] == "Malicious"]

temp_smote = pd.concat([ben, mal], axis=0)

labels = temp_smote['Label']
temp_smote = temp_smote.drop(['Label'], axis=1)

sm = SMOTE(random_state=RANDOM_SEED)

temp_smote, labels = sm.fit_resample(temp_smote, labels)

temp_smote = pd.concat([temp_smote, labels], axis=1)

"""divisão em três subsets de dados com 1/3 de nondoh em cada (maliciosos e benignos são repetidos)"""

nlabels = non['Label']
non = non.drop(['Label'], axis=1)

first_non, temp_non, first_labels, temp_labels = train_test_split(non, nlabels, test_size=0.6666, random_state=RANDOM_SEED)

second_non, third_non, second_labels, third_labels = train_test_split(temp_non, temp_labels, test_size=0.5, random_state=RANDOM_SEED)

first_non = pd.concat([first_non, first_labels], axis=1)
second_non = pd.concat([second_non, second_labels], axis=1)
third_non = pd.concat([third_non, third_labels], axis=1)

df1 = pd.concat([first_non, temp_smote], axis=0)
df2 = pd.concat([second_non, temp_smote], axis=0)
df3 = pd.concat([third_non, temp_smote], axis=0)

"""quase não acredito que acabou"""

df_test.to_csv("test_data.csv", index=False)
df1.to_csv("train_data1.csv", index=False)
df2.to_csv("train_data2.csv", index=False)
df3.to_csv("train_data3.csv", index=False)